<!DOCTYPE html><html lang="en-us" class="__variable_dd5b2f scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/36966cca54120369-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/c890694439b2475b.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/1ea5cf861ee12a80.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/a9b9096fa657c0d0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-8506c16620cf39fb.js"/><script src="/_next/static/chunks/fd9d1056-30760135124e6678.js" async=""></script><script src="/_next/static/chunks/23-44b8024386371b46.js" async=""></script><script src="/_next/static/chunks/main-app-06a10a1bb45617e8.js" async=""></script><script src="/_next/static/chunks/ebde5ed1-51545511fe0d5050.js" async=""></script><script src="/_next/static/chunks/231-34a6a67d2da26855.js" async=""></script><script src="/_next/static/chunks/827-69594f61c16b8a9c.js" async=""></script><script src="/_next/static/chunks/850-ecf153581cc02044.js" async=""></script><script src="/_next/static/chunks/app/layout-54bebb918ae7f176.js" async=""></script><script src="/_next/static/chunks/app/blog/%5B...slug%5D/page-bd056182432da53b.js" async=""></script><script src="https://us.umami.is/script.js" async=""></script><title>提示缓存：10倍更便宜的 LLM Token，原理是什么？</title><meta name="description" content="深入解析 LLM 的提示缓存机制，了解 KV 缓存如何工作，为什么能节省成本并降低延迟，以及 OpenAI 和 Anthropic 不同的缓存策略。"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://blog.mainjay.cloudns.ch/blog/ai/prompt-caching"/><link rel="alternate" type="application/rss+xml" href="https://blog.mainjay.cloudns.ch/feed.xml"/><meta property="og:title" content="提示缓存：10倍更便宜的 LLM Token，原理是什么？"/><meta property="og:description" content="深入解析 LLM 的提示缓存机制，了解 KV 缓存如何工作，为什么能节省成本并降低延迟，以及 OpenAI 和 Anthropic 不同的缓存策略。"/><meta property="og:url" content="https://blog.mainjay.cloudns.ch/blog/ai/prompt-caching"/><meta property="og:site_name" content="MainJayLai Blog"/><meta property="og:locale" content="en_US"/><meta property="og:image" content="https://pngimg.com/uploads/github/github_PNG80.png"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2025-12-31T00:00:00.000Z"/><meta property="article:modified_time" content="2025-12-31T00:00:00.000Z"/><meta property="article:author" content="mainJayLai"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="提示缓存：10倍更便宜的 LLM Token，原理是什么？"/><meta name="twitter:description" content="深入解析 LLM 的提示缓存机制，了解 KV 缓存如何工作，为什么能节省成本并降低延迟，以及 OpenAI 和 Anthropic 不同的缓存策略。"/><meta name="twitter:image" content="https://pngimg.com/uploads/github/github_PNG80.png"/><meta name="next-size-adjust"/><link rel="icon" type="image/png" href="https://mainjaylai.github.io/favicon.png"/><link rel="manifest" href="/static/favicons/manifest.json"/><meta name="msapplication-TileColor" content="#000000"/><meta name="theme-color" media="(prefers-color-scheme: light)" content="#fff"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#000"/><meta name="referrer" content="no-referrer"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@200..900&amp;display=swap" rel="stylesheet"/><link href="https://fonts.googleapis.com/css2?family=ZCOOL+KuaiLe&amp;family=ZCOOL+QingKe+HuangYou&amp;family=ZCOOL+XiaoWei&amp;display=swap" rel="stylesheet"/><script src="https://cdn.jsdelivr.net/gh/ashishagarwal2023/freegptjs@1.0.2/src/freegpt.min.js"></script><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="bg-white pl-[calc(100vw-100%)] text-black antialiased dark:bg-gray-950 dark:text-white"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&false)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}else{c.add('light')}if(e==='light'||e==='dark'||!e)d.style.colorScheme=e||'light'}catch(e){}}()</script><section class="mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0"><div class="flex h-screen flex-col justify-between font-sans"><header class="flex items-center justify-between py-5"><div><a aria-label="Blog" href="/"><div class="flex items-center justify-between"><div class="mr-3"><img alt="logo" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" style="color:transparent" src="https://mainjaylai.github.io/favicon.png"/></div><div class="hidden h-[44px] text-center text-3xl font-semibold leading-10 sm:block">Blog</div></div></a></div><div class="flex items-center space-x-4 leading-5 sm:space-x-6"><a class="navbar-item hidden font-medium text-gray-900 dark:text-gray-100 sm:block" href="/blog">Blog</a><a class="navbar-item hidden font-medium text-gray-900 dark:text-gray-100 sm:block" href="/tags">Tags</a><a target="_blank" rel="noopener noreferrer" href="https://mainjaylai.github.io" class="navbar-item hidden font-medium text-gray-900 dark:text-gray-100 sm:block">About</a><button aria-label="Search"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="h-6 w-6 text-gray-900 dark:text-gray-100"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-5.197-5.197m0 0A7.5 7.5 0 105.196 5.196a7.5 7.5 0 0010.607 10.607z"></path></svg></button><div class="mr-8"><div class="relative inline-block text-left" data-headlessui-state=""><div><button id="headlessui-menu-button-:R5pkqja:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="h-6 w-6 text-gray-900 dark:text-gray-100"><path fill-rule="evenodd" d="M10 2a1 1 0 011 1v1a1 1 0 11-2 0V3a1 1 0 011-1zm4 8a4 4 0 11-8 0 4 4 0 018 0zm-.464 4.95l.707.707a1 1 0 001.414-1.414l-.707-.707a1 1 0 00-1.414 1.414zm2.12-10.607a1 1 0 010 1.414l-.706.707a1 1 0 11-1.414-1.414l.707-.707a1 1 0 011.414 0zM17 11a1 1 0 100-2h-1a1 1 0 100 2h1zm-7 4a1 1 0 011 1v1a1 1 0 11-2 0v-1a1 1 0 011-1zM5.05 6.464A1 1 0 106.465 5.05l-.708-.707a1 1 0 00-1.414 1.414l.707.707zm1.414 8.486l-.707.707a1 1 0 01-1.414-1.414l.707-.707a1 1 0 011.414 1.414zM4 11a1 1 0 100-2H3a1 1 0 000 2h1z" clip-rule="evenodd"></path></svg></button></div></div></div><button aria-label="AI Chat" class="flex items-center justify-center rounded-full p-1.5 text-gray-800 transition-all hover:bg-gray-100 dark:text-gray-200 dark:hover:bg-gray-800"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path><path d="M8 9h8"></path><path d="M8 13h6"></path></svg></button><div class="mr-5"><div class="relative inline-block text-left" data-headlessui-state=""><div><button class="flex items-center" id="headlessui-menu-button-:R6pkqja:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><div class="mr-2">简体中文</div><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1616" width="15" height="15"><path d="M670.72 325.696L511.552 152l87.68-87.872 360.96 389.504L832 453.76v0.64H64V325.76h606.72z m-318.4 382.08l157.248 172.8-94.976 79.552L63.872 579.84l147.712-0.704v-0.128H960v128.768H352.32z" fill="#262626" p-id="1617"></path></svg></button></div></div></div><button aria-label="Toggle Menu" class="sm:hidden"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="h-8 w-8 text-gray-900 dark:text-gray-100"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg></button><div class="opacity-98 fixed left-0 top-0 z-10 h-full w-full transform bg-white duration-300 ease-in-out dark:bg-gray-950 dark:opacity-[0.98] translate-x-full"><div class="flex justify-end"><button class="mr-8 mt-8 h-8 w-8" aria-label="Toggle Menu"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 dark:text-gray-100"><path fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></button></div><nav class="fixed mt-8 h-full"><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 opacity-80 dark:text-gray-100" href="/">Home</a></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 opacity-80 dark:text-gray-100" href="/blog">Blog</a></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 opacity-80 dark:text-gray-100" href="/tags">Tags</a></div><div class="px-12 py-4"><a target="_blank" rel="noopener noreferrer" href="https://mainjaylai.github.io" class="text-2xl font-bold tracking-widest text-gray-900 opacity-80 dark:text-gray-100">About</a></div></nav></div></div></header><main class="mb-auto"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"提示缓存：10倍更便宜的 LLM Token，原理是什么？","datePublished":"2025-12-31T00:00:00.000Z","dateModified":"2025-12-31T00:00:00.000Z","description":"深入解析 LLM 的提示缓存机制，了解 KV 缓存如何工作，为什么能节省成本并降低延迟，以及 OpenAI 和 Anthropic 不同的缓存策略。","image":"https://pngimg.com/uploads/github/github_PNG80.png","url":"https://blog.mainjay.cloudns.ch/blog/ai/prompt-caching","author":[{"@type":"Person","name":"mainJayLai"}]}</script><section class="mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0"><div class="fixed bottom-8 right-8 hidden flex-col gap-3 md:hidden"><button aria-label="Scroll To Comment" class="rounded-full bg-gray-200 p-2 text-gray-500 transition-all hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-400 dark:hover:bg-gray-600"><svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M18 10c0 3.866-3.582 7-8 7a8.841 8.841 0 01-4.083-.98L2 17l1.338-3.123C2.493 12.767 2 11.434 2 10c0-3.866 3.582-7 8-7s8 3.134 8 7zM7 9H5v2h2V9zm8 0h-2v2h2V9zM9 9h2v2H9V9z" clip-rule="evenodd"></path></svg></button><button aria-label="Scroll To Top" class="rounded-full bg-gray-200 p-2 text-gray-500 transition-all hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-400 dark:hover:bg-gray-600"><svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z" clip-rule="evenodd"></path></svg></button></div><article><div><header><div class="space-y-1 border-b border-gray-200 pb-10 text-center dark:border-gray-700"><div class="beautiful-chinese-title"><h1 class="text-3xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 sm:text-4xl sm:leading-10 md:text-5xl md:leading-14">提示缓存：10倍更便宜的 LLM Token，原理是什么？</h1></div><dl><div><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2025-12-31T00:00:00.000Z">December 31, 2025</time></dd></div></dl></div></header><div class="grid-rows-[auto_1fr] divide-y divide-gray-200 pb-8 dark:divide-gray-700 xl:divide-y-0"><div class="divide-y divide-gray-200 dark:divide-gray-700 xl:col-span-3 xl:row-span-2 xl:pb-0"><div class="beautiful-chinese-content prose max-w-none pb-8 pt-10 dark:prose-invert"><h2 class="content-header" id="目录"><a href="#目录" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>目录</h2><ul class=""><li class=""><a href="#引言">引言</a></li><li class=""><a href="#llm-架构速览">LLM 架构速览</a></li><li class=""><a href="#分词从文本到-token">分词：从文本到 Token</a></li><li class=""><a href="#嵌入从-token-到向量">嵌入：从 Token 到向量</a></li><li class=""><a href="#注意力机制缓存发生的地方">注意力机制：缓存发生的地方</a></li><li class=""><a href="#kv-缓存真正的魔法">KV 缓存：真正的魔法</a></li><li class=""><a href="#提示缓存跨请求的-kv-缓存">提示缓存：跨请求的 KV 缓存</a></li><li class=""><a href="#openai-vs-anthropic不同的缓存策略">OpenAI vs Anthropic：不同的缓存策略</a></li><li class=""><a href="#总结">总结</a></li><li class=""><a href="#参考资料">参考资料</a></li></ul><h2 class="content-header" id="引言"><a href="#引言" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>引言</h2><p>使用大型语言模型 (LLM) 的 API 价格通常按 token 收费。每次你向 Claude、GPT-4 或其他模型发送请求时，都需要为输入 token（发送给模型的内容）和输出 token（模型返回的内容）付费。</p><p>但事情有个微妙之处。使用 API 时，你经常会发送包含大量重复内容的消息。也许你每次请求都附带了一个大型系统提示，或者你在对话进行中不断发送整个对话历史。无论哪种情况，你都在为多次处理相同的数据付费。</p><p>为了解决这个问题，LLM 提供商开发了缓存功能。OpenAI 提供<a target="_blank" rel="noopener noreferrer" href="https://platform.openai.com/docs/guides/prompt-caching">提示缓存</a>，Anthropic 则提供<a target="_blank" rel="noopener noreferrer" href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching">扩展缓存</a>。两者都承诺为缓存的 token 提供高达 90% 的折扣以及更低的延迟。</p><p>这就引出了一个问题：LLM 究竟是如何利用缓存的？什么数据被缓存了？为什么它能让处理更便宜？</p><p>本文将探讨这些问题，并让你深入理解这个机制的工作原理。</p><h2 class="content-header" id="llm-架构速览"><a href="#llm-架构速览" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>LLM 架构速览</h2><p>要理解提示缓存如何让 LLM 推理更快、更便宜，我们首先需要对 LLM 进行端到端的概览，考察 transformer 模型的各个步骤及其作用。我们可以将 LLM 分为四个阶段：</p><ol><li><strong>分词器 (Tokenizer)</strong>：将输入文本（字符串）转换为 token ID（整数）列表</li><li><strong>嵌入 (Embedding)</strong>：将 token ID（整数）转换为向量（n 维空间中的浮点数列表）</li><li><strong>变换器 (Transformer)</strong>：接收嵌入向量列表，输出另一个嵌入向量列表</li><li><strong>输出 (Output)</strong>：将嵌入转回词汇表上的 token 概率</li></ol><p>让我们逐一探索这些阶段，以便了解缓存实际发生的位置以及它如何提高性能。</p><h2 class="content-header" id="分词从文本到-token"><a href="#分词从文本到-token" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>分词：从文本到 Token</h2><p>大语言模型并不直接处理我们读写的原始文本。相反，它们处理的是 <strong>token</strong>。token 可以是单个字符、词素或整个单词，这取决于模型所使用的分词器。</p><p>所有 token 被编译成一个 <strong>词汇表</strong>——模型知道的所有 token 的完整列表。词汇表通常有数万个条目，每个条目都有唯一的整数 ID。这个概念并不特别新奇：就像你从电子表格中导出数据时，可能会将类别列转换为类别 ID。</p><p>为了让你直观感受这个概念：</p><div class="w-full overflow-x-auto"><table><thead><tr><th>Token</th><th>Token ID</th></tr></thead><tbody><tr><td>The</td><td>464</td></tr><tr><td>cat</td><td>2857</td></tr><tr><td>sat</td><td>8714</td></tr><tr><td>on</td><td>319</td></tr><tr><td>the</td><td>279</td></tr><tr><td>mat</td><td>5765</td></tr></tbody></table></div><p>当句子 &quot;The cat sat on the mat&quot; 经过分词器处理后：</p><div class="relative"><pre><code class="code-highlight language-js"><span class="code-line"><span class="token string">&quot;The cat sat on the mat&quot;</span> → <span class="token punctuation">[</span><span class="token number">464</span><span class="token punctuation">,</span> <span class="token number">2857</span><span class="token punctuation">,</span> <span class="token number">8714</span><span class="token punctuation">,</span> <span class="token number">319</span><span class="token punctuation">,</span> <span class="token number">279</span><span class="token punctuation">,</span> <span class="token number">5765</span><span class="token punctuation">]</span>
</span></code></pre></div><h2 class="content-header" id="嵌入从-token-到向量"><a href="#嵌入从-token-到向量" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>嵌入：从 Token 到向量</h2><p>一旦我们有了 token ID 序列，下一步就是将每个 ID 转换为 <strong>嵌入向量</strong>。</p><p>嵌入是表示一段数据（在这里是一个 token）的实数向量。你可以把它想象成 token 语义含义的数学表示，捕获了模型所理解的关于这个 token 在语言中如何使用的一切。</p><p>更直白地说：<strong>嵌入只是一个数字列表</strong>。每个 token 都有自己的嵌入，嵌入包含相同数量的数字（即维度）。现代 LLM 通常使用大约 1536 到 8192 个维度。</p><p>例如，一个非常简化的 3 维嵌入可能是：</p><div class="w-full overflow-x-auto"><table><thead><tr><th>Token</th><th>嵌入 (3维示例)</th></tr></thead><tbody><tr><td>cat</td><td>[0.12, -0.45, 0.78]</td></tr><tr><td>dog</td><td>[0.11, -0.42, 0.80]</td></tr><tr><td>mat</td><td>[0.85, 0.23, -0.12]</td></tr></tbody></table></div><p>注意 &quot;cat&quot; 和 &quot;dog&quot; 的嵌入相似——因为它们在语义上相关（都是动物）。&quot;mat&quot; 则完全不同。实际的嵌入要复杂得多，但原理相同。</p><p>到这个阶段，我们输入到 LLM 的句子从一个字符串变成了一个 <strong>嵌入矩阵</strong>：每行代表一个 token，列代表嵌入维度。</p><h2 class="content-header" id="注意力机制缓存发生的地方"><a href="#注意力机制缓存发生的地方" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>注意力机制：缓存发生的地方</h2><p>这就是精华所在。Transformer 的核心是 <strong>自注意力机制 (Self-Attention)</strong>，它允许模型中的每个 token 关注序列中的其他 token 并混合信息。这是 LLM 真正进行&quot;思考&quot;和&quot;推理&quot;的地方。</p><h3 class="content-header" id="qkv-矩阵"><a href="#qkv-矩阵" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Q、K、V 矩阵</h3><p>注意力机制使用三个关键概念：<strong>Query (查询)</strong>、<strong>Key (键)</strong> 和 <strong>Value (值)</strong>。</p><p>可以这样理解：</p><ul><li><strong>Query (Q)</strong>：代表&quot;我在寻找什么信息？&quot;</li><li><strong>Key (K)</strong>：代表&quot;我有什么信息可以提供？&quot;</li><li><strong>Value (V)</strong>：代表&quot;如果你问我，这是我实际提供的信息&quot;</li></ul><p>对于输入中的每个 token 嵌入，我们通过与三个权重矩阵相乘来生成 Q、K、V 向量：</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>=</mo><mi>X</mi><mo>⋅</mo><msub><mi>W</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">Q = X \cdot W_Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8778em;vertical-align:-.1944em"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.9694em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3283em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>=</mo><mi>X</mi><mo>⋅</mo><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">K = X \cdot W_K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3283em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mathnormal mtight" style="margin-right:.07153em">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>=</mo><mi>X</mi><mo>⋅</mo><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">V = X \cdot W_V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3283em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mathnormal mtight" style="margin-right:.22222em">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></p><p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span></span></span></span> 是输入嵌入矩阵，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">W_Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.9694em;vertical-align:-.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3283em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">W_K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3283em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mathnormal mtight" style="margin-right:.07153em">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">W_V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3283em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mathnormal mtight" style="margin-right:.22222em">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 是模型在训练过程中学到的权重矩阵。</p><h3 class="content-header" id="注意力分数计算"><a href="#注意力分数计算" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>注意力分数计算</h3><p>计算出 Q、K、V 后，注意力机制执行以下计算：</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo fence="true">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1.8em;vertical-align:-.65em"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mspace" style="margin-right:.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0895em"><span style="top:-2.5864em"><span class="pstrut" style="height:3em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mtight sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8622em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mtight" style="padding-left:.833em"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3448em"><span style="top:-2.3488em;margin-left:0;margin-right:.0714em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.1512em"><span></span></span></span></span></span></span></span></span><span style="top:-2.8222em"><span class="pstrut" style="height:3em"></span><span class="mtight hide-tail" style="min-width:.853em;height:1.08em"><svg viewBox="0 0 400000 1080" xmlns="http://www.w3.org/2000/svg" height="1.08em" preserveAspectRatio="xMinYMin slice" width="400em"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.1778em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.4461em"><span class="pstrut" style="height:3em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.07153em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.9191em"><span style="top:-2.931em;margin-right:.0714em"><span class="pstrut" style="height:2.5em"></span><span class="mtight sizing reset-size3 size1"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.538em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0"><span class="delimsizing size2">)</span></span></span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span></span></span></span></p><p>这个公式做了什么？</p><ol><li><strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-.1944em"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:.07153em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span></span></span></span></strong>：计算每个 query 与每个 key 的点积，得到注意力分数</li><li><strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-.1828em"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8572em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:.833em"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:.853em;height:1.08em"><svg viewBox="0 0 400000 1080" xmlns="http://www.w3.org/2000/svg" height="1.08em" preserveAspectRatio="xMinYMin slice" width="400em"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.1828em"><span></span></span></span></span></span></span></span></span></strong>：缩放因子，防止分数过大</li><li><strong>softmax</strong>：将分数转换为概率分布（和为 1）</li><li><strong>乘以 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span></span></span></span></strong>：用注意力权重加权求和所有 value</li></ol><h3 class="content-header" id="因果掩码"><a href="#因果掩码" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>因果掩码</h3><p>对于生成式 LLM（如 GPT、Claude），还有一个关键细节：<strong>因果掩码 (Causal Mask)</strong>。</p><p>在生成文本时，模型一次生成一个 token。关键规则是：<strong>每个 token 只能看到它之前的 token</strong>，不能&quot;偷看&quot;未来的 token。</p><p>为了实现这一点，在计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-.1944em"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:.07153em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span></span></span></span> 后、应用 softmax 之前，我们添加一个下三角掩码，将未来位置的分数设为负无穷。负无穷经过 softmax 后变成 0，有效地阻止了信息从未来流向过去。</p><h2 class="content-header" id="kv-缓存真正的魔法"><a href="#kv-缓存真正的魔法" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>KV 缓存：真正的魔法</h2><p>现在我们终于可以理解提示缓存的核心机制了。</p><h3 class="content-header" id="为什么需要-kv-缓存"><a href="#为什么需要-kv-缓存" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>为什么需要 KV 缓存？</h3><p>考虑这样一个场景：你正在与 LLM 进行对话，每次发送新消息时都需要包含之前的对话历史。</p><p>假设对话历史有 1000 个 token，你新增了 10 个 token。传统方式下，模型需要：</p><ol><li>对所有 1010 个 token 重新计算 Q、K、V 矩阵</li><li>计算完整的注意力分数</li><li>生成输出</li></ol><p>但这里有个洞察：<strong>之前 1000 个 token 的 K 和 V 矩阵不会改变</strong>！</p><p>因为计算 K 和 V 只依赖于 token 本身和固定的权重矩阵。只要输入 token 相同，输出的 K、V 就相同。</p><h3 class="content-header" id="kv-缓存如何工作"><a href="#kv-缓存如何工作" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>KV 缓存如何工作</h3><p>有了这个洞察，KV 缓存的思路就很清晰了：</p><ol><li><strong>首次处理提示时</strong>：计算并存储所有 token 的 K 和 V 矩阵</li><li><strong>生成新 token 时</strong>：只为新 token 计算 K 和 V，然后将其追加到缓存中</li><li><strong>计算注意力时</strong>：新 token 的 Q 与所有缓存的 K 计算注意力分数，然后用这些分数加权所有缓存的 V</li></ol><p>这带来了巨大的计算节省：</p><ul><li><strong>无缓存</strong>：每生成一个新 token，都要重新处理所有之前的 token</li><li><strong>有缓存</strong>：每生成一个新 token，只需处理那一个 token</li></ul><p>复杂度从 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 降低到了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span>，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">n</span></span></span></span> 是序列长度。</p><h3 class="content-header" id="实际数据量"><a href="#实际数据量" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>实际数据量</h3><p>让我们计算一下缓存了多少数据。假设：</p><ul><li>模型维度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>=</mo><mn>4096</mn></mrow><annotation encoding="application/x-tex">d = 4096</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">4096</span></span></span></span>（常见于 GPT-4 级别模型）</li><li>32 层</li><li>32 个注意力头</li><li>每个头维度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">d_k = 128</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="mtight sizing reset-size6 size3"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">128</span></span></span></span></li></ul><p>对于 1024 个 token 的缓存：</p><ul><li>每个 token：K 和 V 各 4096 个浮点数 × 32 层 = 262,144 个浮点数</li><li>1024 个 token：约 2.68 亿个浮点数</li><li>以 FP16 存储：约 512 MB</li></ul><p>这就是为什么 LLM 推理需要大量 GPU 显存——缓存本身就占用了相当大的空间。</p><h2 class="content-header" id="提示缓存跨请求的-kv-缓存"><a href="#提示缓存跨请求的-kv-缓存" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>提示缓存：跨请求的 KV 缓存</h2><p>标准的 KV 缓存只在单次请求内有效。但 LLM 提供商更进一步：<strong>跨请求持久化缓存</strong>。</p><p>这就是 OpenAI 和 Anthropic 的&quot;提示缓存&quot;功能。</p><h3 class="content-header" id="工作原理"><a href="#工作原理" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>工作原理</h3><ol><li><strong>缓存键</strong>：系统使用提示内容的哈希值作为缓存键</li><li><strong>缓存匹配</strong>：新请求到来时，检查提示前缀是否与缓存匹配</li><li><strong>缓存命中</strong>：如果匹配，直接使用缓存的 K、V 矩阵，跳过计算</li><li><strong>缓存失效</strong>：经过一段时间后缓存自动失效（通常 5-10 分钟）</li></ol><h3 class="content-header" id="为什么能省钱"><a href="#为什么能省钱" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>为什么能省钱？</h3><p>计算 K、V 矩阵是 LLM 推理中最昂贵的部分之一。当你的请求命中缓存时：</p><ul><li><strong>减少计算</strong>：GPU 不需要为缓存的 token 进行矩阵运算</li><li><strong>降低延迟</strong>：直接使用预计算的结果</li><li><strong>节省成本</strong>：提供商将节省的成本以折扣形式传递给用户</li></ul><p>这就是为什么缓存的 token 可以便宜 50-90%。</p><h2 class="content-header" id="openai-vs-anthropic不同的缓存策略"><a href="#openai-vs-anthropic不同的缓存策略" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>OpenAI vs Anthropic：不同的缓存策略</h2><p>虽然底层原理相同，但两家公司的实现策略有显著差异。</p><h3 class="content-header" id="openai自动缓存"><a href="#openai自动缓存" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>OpenAI：自动缓存</h3><p>OpenAI 的提示缓存是 <strong>完全自动</strong> 的：</p><ul><li>自动检测可缓存的提示前缀</li><li>对开发者完全透明</li><li>缓存持续约 5-10 分钟</li><li>最小缓存单位：1024 tokens</li><li>缓存命中时享受 50% 折扣</li></ul><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token comment"># OpenAI 无需任何改动，缓存自动工作</span>
</span><span class="code-line">response <span class="token operator">=</span> client<span class="token punctuation">.</span>chat<span class="token punctuation">.</span>completions<span class="token punctuation">.</span>create<span class="token punctuation">(</span>
</span><span class="code-line">    model<span class="token operator">=</span><span class="token string">&quot;gpt-4&quot;</span><span class="token punctuation">,</span>
</span><span class="code-line">    messages<span class="token operator">=</span><span class="token punctuation">[</span>
</span><span class="code-line">        <span class="token punctuation">{</span><span class="token string">&quot;role&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;system&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;content&quot;</span><span class="token punctuation">:</span> very_long_system_prompt<span class="token punctuation">}</span><span class="token punctuation">,</span>
</span><span class="code-line">        <span class="token punctuation">{</span><span class="token string">&quot;role&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;user&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;content&quot;</span><span class="token punctuation">:</span> user_message<span class="token punctuation">}</span>
</span><span class="code-line">    <span class="token punctuation">]</span>
</span><span class="code-line"><span class="token punctuation">)</span>
</span></code></pre></div><h3 class="content-header" id="anthropic显式控制"><a href="#anthropic显式控制" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Anthropic：显式控制</h3><p>Anthropic 让开发者 <strong>显式标记</strong> 要缓存的内容：</p><ul><li>使用 <code class="custom-code">cache_control</code> 参数指定缓存点</li><li>最多可设置 4 个缓存断点</li><li>更细粒度的控制</li><li>缓存命中时享受 90% 折扣</li><li>写入缓存时有额外费用（约 25%）</li></ul><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token comment"># Anthropic 需要显式标记缓存</span>
</span><span class="code-line">response <span class="token operator">=</span> client<span class="token punctuation">.</span>messages<span class="token punctuation">.</span>create<span class="token punctuation">(</span>
</span><span class="code-line">    model<span class="token operator">=</span><span class="token string">&quot;claude-sonnet-4-20250514&quot;</span><span class="token punctuation">,</span>
</span><span class="code-line">    system<span class="token operator">=</span><span class="token punctuation">[</span>
</span><span class="code-line">        <span class="token punctuation">{</span>
</span><span class="code-line">            <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;text&quot;</span><span class="token punctuation">,</span>
</span><span class="code-line">            <span class="token string">&quot;text&quot;</span><span class="token punctuation">:</span> very_long_system_prompt<span class="token punctuation">,</span>
</span><span class="code-line">            <span class="token string">&quot;cache_control&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;ephemeral&quot;</span><span class="token punctuation">}</span>  <span class="token comment"># 显式缓存</span>
</span><span class="code-line">        <span class="token punctuation">}</span>
</span><span class="code-line">    <span class="token punctuation">]</span><span class="token punctuation">,</span>
</span><span class="code-line">    messages<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">&quot;role&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;user&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;content&quot;</span><span class="token punctuation">:</span> user_message<span class="token punctuation">}</span><span class="token punctuation">]</span>
</span><span class="code-line"><span class="token punctuation">)</span>
</span></code></pre></div><h3 class="content-header" id="如何选择"><a href="#如何选择" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>如何选择？</h3><div class="w-full overflow-x-auto"><table><thead><tr><th>场景</th><th>OpenAI 方案</th><th>Anthropic 方案</th></tr></thead><tbody><tr><td>简单用例</td><td>零配置，自动生效</td><td>需要手动添加缓存标记</td></tr><tr><td>大型系统提示</td><td>自动缓存前缀</td><td>可精确控制缓存边界</td></tr><tr><td>成本优化</td><td>50% 折扣，较简单</td><td>90% 折扣，但有写入成本</td></tr><tr><td>复杂工作流</td><td>控制较少</td><td>更灵活的缓存策略</td></tr></tbody></table></div><h2 class="content-header" id="总结"><a href="#总结" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>总结</h2><p>提示缓存的核心原理可以总结为：</p><ol><li><strong>LLM 推理的瓶颈</strong>：计算注意力机制中的 K、V 矩阵非常昂贵</li><li><strong>KV 缓存的洞察</strong>：对于相同的输入 token，K、V 矩阵的计算结果相同</li><li><strong>跨请求持久化</strong>：将 KV 缓存扩展到多个 API 请求之间共享</li><li><strong>成本节省</strong>：跳过重复计算 = 更少的 GPU 时间 = 更低的价格</li></ol><p>理解这个机制后，你可以：</p><ul><li><strong>优化提示结构</strong>：将静态内容放在前面，动态内容放在后面</li><li><strong>合理使用系统提示</strong>：大型系统提示是缓存的理想候选</li><li><strong>选择合适的提供商</strong>：根据你的需求选择自动或手动缓存策略</li></ul><p>下次当你看到 API 账单上的&quot;缓存 token&quot;折扣时，你就知道背后发生了什么了。</p><hr/><h2 class="content-header" id="参考资料"><a href="#参考资料" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="h-5 linkicon w-5" fill="currentColor"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>参考资料</h2><ul><li><a target="_blank" rel="noopener noreferrer" href="https://ngrok.com/blog/prompt-caching">Prompt caching: 10x cheaper LLM tokens, but how?</a> - Sam Rose, ngrok</li><li><a target="_blank" rel="noopener noreferrer" href="https://platform.openai.com/docs/guides/prompt-caching">OpenAI Prompt Caching Guide</a></li><li><a target="_blank" rel="noopener noreferrer" href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching">Anthropic Prompt Caching Documentation</a></li><li><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> - Vaswani et al.</li></ul></div></div><div class="pb-6 pt-6 text-center text-gray-700 dark:text-gray-300" id="comment"></div><footer><div class="flex flex-col text-sm font-medium sm:flex-row sm:justify-between sm:text-base"><div class="pt-4 xl:pt-8"><a class="text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" aria-label="Previous post: 从文本到词元：分词管道的工作原理" href="/blog/ai/tokenization-pipeline">← <!-- -->从文本到词元：分词管道的工作原理</a></div></div></footer></div></div></article></section></main><footer><div class="mt-16 flex flex-col items-center"><div class="mb-3 flex space-x-4"><a class="text-sm !text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="mailto:mainjaylai@outlook.com"><span class="sr-only">mail</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" class="fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 dark:hover:text-primary-400 h-6 w-6"><path d="M2.003 5.884L10 9.882l7.997-3.998A2 2 0 0016 4H4a2 2 0 00-1.997 1.884z"></path><path d="M18 8.118l-8 4-8-4V14a2 2 0 002 2h12a2 2 0 002-2V8.118z"></path></svg></a><a class="text-sm !text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://github.com/mainjaylai"><span class="sr-only">github</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 dark:hover:text-primary-400 h-6 w-6"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></a><a class="text-sm !text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://gitlab.com/JayMain"><span class="sr-only">gitlab</span><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 dark:hover:text-primary-400 h-6 w-6" width="200" height="200"><path d="M1022.08 579.712l-57.258667-176.426667-113.664-349.397333a19.413333 19.413333 0 0 0-36.992 0L700.501333 403.2H323.498667L209.877333 53.888C204.074667 35.84 178.56 35.84 172.8 53.76L59.136 403.157333 1.877333 579.712a39.424 39.424 0 0 0 14.122667 43.648L512 983.637333l496-360.234666a39.253333 39.253333 0 0 0 14.08-43.690667"></path></svg></a><a class="text-sm !text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://gitee.com/lmj2001"><span class="sr-only">gitee</span><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 dark:hover:text-primary-400 h-6 w-6" width="200" height="200"><path d="M512 992C246.895625 992 32 777.104375 32 512S246.895625 32 512 32s480 214.895625 480 480-214.895625 480-480 480z m242.9521875-533.3278125h-272.56875a23.7121875 23.7121875 0 0 0-23.71125 23.7121875l-0.024375 59.255625c0 13.08 10.6078125 23.7121875 23.6878125 23.7121875h165.96c13.104375 0 23.7121875 10.6078125 23.7121875 23.6878125v11.855625a71.1121875 71.1121875 0 0 1-71.1121875 71.1121875h-225.215625a23.7121875 23.7121875 0 0 1-23.6878125-23.7121875V423.1278125a71.1121875 71.1121875 0 0 1 71.0878125-71.1121875h331.824375a23.7121875 23.7121875 0 0 0 23.6878125-23.71125l0.0721875-59.2565625a23.7121875 23.7121875 0 0 0-23.68875-23.7121875H423.08a177.76875 177.76875 0 0 0-177.76875 177.7921875V754.953125c0 13.1034375 10.60875 23.7121875 23.713125 23.7121875h349.63125a159.984375 159.984375 0 0 0 159.984375-159.984375V482.36a23.7121875 23.7121875 0 0 0-23.7121875-23.6878125z"></path></svg></a></div><div class="mb-2 flex space-x-2 text-sm text-gray-500 dark:text-gray-400"><div>MainJayLai</div><div> • </div><div>© 2025</div><div> • </div><a href="/">MainJayLai Blog</a></div></div></footer></div></section><script src="/_next/static/chunks/webpack-8506c16620cf39fb.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/36966cca54120369-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/c890694439b2475b.css\",\"style\"]\n3:HL[\"/_next/static/css/1ea5cf861ee12a80.css\",\"style\"]\n4:HL[\"/_next/static/css/a9b9096fa657c0d0.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"5:I[5751,[],\"\"]\n8:I[9275,[],\"\"]\na:I[1343,[],\"\"]\nb:I[8700,[\"599\",\"static/chunks/ebde5ed1-51545511fe0d5050.js\",\"231\",\"static/chunks/231-34a6a67d2da26855.js\",\"827\",\"static/chunks/827-69594f61c16b8a9c.js\",\"850\",\"static/chunks/850-ecf153581cc02044.js\",\"185\",\"static/chunks/app/layout-54bebb918ae7f176.js\"],\"ThemeProviders\"]\nc:I[4080,[\"599\",\"static/chunks/ebde5ed1-51545511fe0d5050.js\",\"231\",\"static/chunks/231-34a6a67d2da26855.js\",\"827\",\"static/chunks/827-69594f61c16b8a9c.js\",\"850\",\"static/chunks/850-ecf153581cc02044.js\",\"185\",\"static/chunks/app/layout-54bebb918ae7f176.js\"],\"\"]\nd:I[9032,[\"599\",\"static/chunks/ebde5ed1-51545511fe0d5050.js\",\"231\",\"static/chunks/231-34a6a67d2da26855.js\",\"827\",\"static/chunks/827-69594f61c16b8a9c.js\",\"850\",\"static/chunks/850-ecf153581cc02044.js\",\"185\",\"static/chunks/app/layout-54bebb918ae7f176.js\"],\"KBarSearchProvider\"]\ne:I[231,[\"231\",\"static/chunks/231-34a6a67d2da26855.js\",\"827\",\"static/chunks/827-69594f61c16b8a9c.js\",\"797\",\"static/chunks/app/blog/%5B...slug%5D/page-bd056182432da53b.js\"],\"\"]\nf:I[8173,[\"231\",\"static/chunks/231-34a6a67d2da26855.js\",\"827\",\"static/chunks/827-69594f61c16b8a9c.js\",\"797\",\"static/chunks/app/blog/%5B...slug%5D/page-bd056182432da53b.js\"],\"Image\"]\n10:I[509,[\"599\",\"static/chunks/ebde5ed1-51545511fe0d5050.js\",\"231\",\"static/chunks/231-34a6a67d2da26855.js\",\"827\",\"static/chunks/827-69594f61c16b8a9c.js\",\"850\",\"static/chunks/850-ecf153581cc02044.js\",\"185\",\"static/chunks/app/layout-54bebb918ae7f176.js\"],\"KBarButton\"]\n11:I[1398,[\"599\",\"static/chunks/ebde5ed1-51545511fe0d5050.js\",\"231\",\"static/chunks/231-34a6a67d2da26855.js\",\"827\",\"static/chunks/827-69594f61c16b8a9c.js\",\"850\",\"static/chunks/850-ecf153581cc02044.js\",\"185\",\"static/chunks/app/layout-54bebb918ae7f176.js\"],\"default\"]\n12:I[7606,[\"599\",\"static/chunks/ebde5ed1-51545511fe0d5050.js\",\"231\",\"static/chunks/231-34a6a67d2da26855.js\",\"827\",\"static/chunks/827-69594f61c16b8a9c.js\",\"850\",\"static/chunks/850-ecf153581cc02044.js\",\"185\",\"static/chunks/app/layout-54bebb918ae7f176.js\"],\"default\"]\n13:I[7510,[\"599\",\"static/chunks/ebde5ed1-5"])</script><script>self.__next_f.push([1,"1545511fe0d5050.js\",\"231\",\"static/chunks/231-34a6a67d2da26855.js\",\"827\",\"static/chunks/827-69594f61c16b8a9c.js\",\"850\",\"static/chunks/850-ecf153581cc02044.js\",\"185\",\"static/chunks/app/layout-54bebb918ae7f176.js\"],\"default\"]\n14:I[8976,[\"599\",\"static/chunks/ebde5ed1-51545511fe0d5050.js\",\"231\",\"static/chunks/231-34a6a67d2da26855.js\",\"827\",\"static/chunks/827-69594f61c16b8a9c.js\",\"850\",\"static/chunks/850-ecf153581cc02044.js\",\"185\",\"static/chunks/app/layout-54bebb918ae7f176.js\"],\"default\"]\n16:I[6130,[],\"\"]\n9:[\"slug\",\"ai/prompt-caching\",\"c\"]\n17:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/c890694439b2475b.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/1ea5cf861ee12a80.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"buildId\":\"AO1lBacSEqBfLSl9iFzzQ\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/blog/ai/prompt-caching\",\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"ai/prompt-caching\",\"c\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"ai\\\",\\\"prompt-caching\\\"]}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"ai/prompt-caching\",\"c\"],{\"children\":[\"__PAGE__\",{},[[\"$L6\",\"$L7\"],null],null]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",\"$9\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/a9b9096fa657c0d0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]}],null]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"en-us\",\"className\":\"__variable_dd5b2f scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"href\":\"https://mainjaylai.github.io/favicon.png\"}],[\"$\",\"link\",null,{\"rel\":\"manifest\",\"href\":\"/static/favicons/manifest.json\"}],[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#000000\"}],[\"$\",\"link\",null,{\"href\":\"https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@200..900\u0026display=swap\",\"rel\":\"stylesheet\"}],[\"$\",\"link\",null,{\"href\":\"https://fonts.googleapis.com/css2?family=ZCOOL+KuaiLe\u0026family=ZCOOL+QingKe+HuangYou\u0026family=ZCOOL+XiaoWei\u0026display=swap\",\"rel\":\"stylesheet\"}],[\"$\",\"script\",null,{\"src\":\"https://us.umami.is/script.js\",\"async\":true}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: light)\",\"content\":\"#fff\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: dark)\",\"content\":\"#000\"}],[\"$\",\"meta\",null,{\"name\":\"referrer\",\"content\":\"no-referrer\"}],[\"$\",\"script\",null,{\"src\":\"https://cdn.jsdelivr.net/gh/ashishagarwal2023/freegptjs@1.0.2/src/freegpt.min.js\"}],[\"$\",\"link\",null,{\"rel\":\"alternate\",\"type\":\"application/rss+xml\",\"href\":\"/feed.xml\"}]]}],[\"$\",\"body\",null,{\"className\":\"bg-white pl-[calc(100vw-100%)] text-black antialiased dark:bg-gray-950 dark:text-white\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"$Lb\",null,{\"children\":[[\"$undefined\",\"$undefined\",\"$undefined\",[\"$\",\"$Lc\",null,{\"async\":true,\"defer\":true,\"data-website-id\":\"bbe21cb3-3de3-4ba7-b6de-453053bc6ae8\",\"src\":\"https://us.umami.is/script.js\"}],\"$undefined\",\"$undefined\"],[\"$\",\"section\",null,{\"className\":\"mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex h-screen flex-col justify-between font-sans\",\"children\":[[\"$\",\"$Ld\",null,{\"kbarConfig\":{\"searchDocumentsPath\":\"/search.json\"},\"children\":[[\"$\",\"header\",null,{\"className\":\"flex items-center justify-between py-5\",\"children\":[[\"$\",\"div\",null,{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/\",\"aria-label\":\"Blog\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center justify-between\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mr-3\",\"children\":[\"$\",\"$Lf\",null,{\"src\":\"https://mainjaylai.github.io/favicon.png\",\"width\":44,\"height\":44,\"alt\":\"logo\"}]}],[\"$\",\"div\",null,{\"className\":\"hidden h-[44px] text-center text-3xl font-semibold leading-10 sm:block\",\"children\":\"Blog\"}]]}]}]}],[\"$\",\"div\",null,{\"className\":\"flex items-center space-x-4 leading-5 sm:space-x-6\",\"children\":[[[\"$\",\"$Le\",null,{\"href\":\"/blog\",\"className\":\"navbar-item hidden font-medium text-gray-900 dark:text-gray-100 sm:block\",\"children\":\"Blog\"}],[\"$\",\"$Le\",null,{\"href\":\"/tags\",\"className\":\"navbar-item hidden font-medium text-gray-900 dark:text-gray-100 sm:block\",\"children\":\"Tags\"}],[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://mainjaylai.github.io\",\"className\":\"navbar-item hidden font-medium text-gray-900 dark:text-gray-100 sm:block\",\"children\":\"About\"}]],[\"$\",\"$L10\",null,{\"aria-label\":\"Search\",\"children\":[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"none\",\"viewBox\":\"0 0 24 24\",\"strokeWidth\":1.5,\"stroke\":\"currentColor\",\"className\":\"h-6 w-6 text-gray-900 dark:text-gray-100\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"d\":\"M21 21l-5.197-5.197m0 0A7.5 7.5 0 105.196 5.196a7.5 7.5 0 0010.607 10.607z\"}]}]}],[\"$\",\"$L11\",null,{}],[\"$\",\"$L12\",null,{}],[\"$\",\"$L13\",null,{}],[\"$\",\"$L14\",null,{}]]}]]}],[\"$\",\"main\",null,{\"className\":\"mb-auto\",\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"flex flex-col items-start justify-start md:mt-24 md:flex-row md:items-center md:justify-center md:space-x-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"space-x-2 pb-8 pt-6 md:space-y-5\",\"children\":[\"$\",\"h1\",null,{\"className\":\"text-6xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 md:border-r-2 md:px-6 md:text-8xl md:leading-14\",\"children\":\"404\"}]}],[\"$\",\"div\",null,{\"className\":\"max-w-md\",\"children\":[[\"$\",\"p\",null,{\"className\":\"mb-4 text-xl font-bold leading-normal md:text-2xl\",\"children\":\"Sorry we couldn't find this page.\"}],[\"$\",\"p\",null,{\"className\":\"mb-8\",\"children\":\"But dont worry, you can find plenty of other things on our homepage.\"}],[\"$\",\"$Le\",null,{\"href\":\"/\",\"className\":\"focus:shadow-outline-blue inline rounded-lg border border-transparent bg-blue-600 px-4 py-2 text-sm font-medium leading-5 text-white shadow transition-colors duration-150 hover:bg-blue-700 focus:outline-none dark:hover:bg-blue-500\",\"children\":\"Back to homepage\"}]]}]]}],\"notFoundStyles\":[],\"styles\":null}]}]]}],[\"$\",\"footer\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"mt-16 flex flex-col items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-3 flex space-x-4\",\"children\":[[\"$\",\"a\",null,{\"className\":\"text-sm !text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"mailto:mainjaylai@outlook.com\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"mail\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 20 20\",\"className\":\"fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 dark:hover:text-primary-400 h-6 w-6\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M2.003 5.884L10 9.882l7.997-3.998A2 2 0 0016 4H4a2 2 0 00-1.997 1.884z\"}],[\"$\",\"path\",null,{\"d\":\"M18 8.118l-8 4-8-4V14a2 2 0 002 2h12a2 2 0 002-2V8.118z\"}]]}]]}],[\"$\",\"a\",null,{\"className\":\"text-sm !text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://github.com/mainjaylai\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"github\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 dark:hover:text-primary-400 h-6 w-6\",\"children\":[\"$\",\"path\",null,{\"d\":\"M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12\"}]}]]}],[\"$\",\"a\",null,{\"className\":\"text-sm !text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://gitlab.com/JayMain\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"gitlab\"}],[\"$\",\"svg\",null,{\"viewBox\":\"0 0 1024 1024\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 dark:hover:text-primary-400 h-6 w-6\",\"width\":\"200\",\"height\":\"200\",\"children\":[\"$\",\"path\",null,{\"d\":\"M1022.08 579.712l-57.258667-176.426667-113.664-349.397333a19.413333 19.413333 0 0 0-36.992 0L700.501333 403.2H323.498667L209.877333 53.888C204.074667 35.84 178.56 35.84 172.8 53.76L59.136 403.157333 1.877333 579.712a39.424 39.424 0 0 0 14.122667 43.648L512 983.637333l496-360.234666a39.253333 39.253333 0 0 0 14.08-43.690667\"}]}]]}],[\"$\",\"a\",null,{\"className\":\"text-sm !text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://gitee.com/lmj2001\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"gitee\"}],[\"$\",\"svg\",null,{\"viewBox\":\"0 0 1024 1024\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 dark:hover:text-primary-400 h-6 w-6\",\"width\":\"200\",\"height\":\"200\",\"children\":[\"$\",\"path\",null,{\"d\":\"M512 992C246.895625 992 32 777.104375 32 512S246.895625 32 512 32s480 214.895625 480 480-214.895625 480-480 480z m242.9521875-533.3278125h-272.56875a23.7121875 23.7121875 0 0 0-23.71125 23.7121875l-0.024375 59.255625c0 13.08 10.6078125 23.7121875 23.6878125 23.7121875h165.96c13.104375 0 23.7121875 10.6078125 23.7121875 23.6878125v11.855625a71.1121875 71.1121875 0 0 1-71.1121875 71.1121875h-225.215625a23.7121875 23.7121875 0 0 1-23.6878125-23.7121875V423.1278125a71.1121875 71.1121875 0 0 1 71.0878125-71.1121875h331.824375a23.7121875 23.7121875 0 0 0 23.6878125-23.71125l0.0721875-59.2565625a23.7121875 23.7121875 0 0 0-23.68875-23.7121875H423.08a177.76875 177.76875 0 0 0-177.76875 177.7921875V754.953125c0 13.1034375 10.60875 23.7121875 23.713125 23.7121875h349.63125a159.984375 159.984375 0 0 0 159.984375-159.984375V482.36a23.7121875 23.7121875 0 0 0-23.7121875-23.6878125z\"}]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"mb-2 flex space-x-2 text-sm text-gray-500 dark:text-gray-400\",\"children\":[[\"$\",\"div\",null,{\"children\":\"MainJayLai\"}],[\"$\",\"div\",null,{\"children\":\" • \"}],[\"$\",\"div\",null,{\"children\":\"© 2025\"}],[\"$\",\"div\",null,{\"children\":\" • \"}],[\"$\",\"$Le\",null,{\"href\":\"/\",\"children\":\"MainJayLai Blog\"}]]}]]}]}]]}]}]]}]}]]}],null],null],\"couldBeIntercepted\":false,\"initialHead\":[false,\"$L15\"],\"globalErrorComponent\":\"$16\",\"missingSlots\":\"$W17\"}]]\n"])</script><script>self.__next_f.push([1,"18:I[4347,[\"231\",\"static/chunks/231-34a6a67d2da26855.js\",\"827\",\"static/chunks/827-69594f61c16b8a9c.js\",\"797\",\"static/chunks/app/blog/%5B...slug%5D/page-bd056182432da53b.js\"],\"default\"]\n19:I[408,[\"231\",\"static/chunks/231-34a6a67d2da26855.js\",\"827\",\"static/chunks/827-69594f61c16b8a9c.js\",\"797\",\"static/chunks/app/blog/%5B...slug%5D/page-bd056182432da53b.js\"],\"default\"]\n1a:I[9629,[\"231\",\"static/chunks/231-34a6a67d2da26855.js\",\"827\",\"static/chunks/827-69594f61c16b8a9c.js\",\"797\",\"static/chunks/app/blog/%5B...slug%5D/page-bd056182432da53b.js\"],\"default\"]\n"])</script><script>self.__next_f.push([1,"7:[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BlogPosting\\\",\\\"headline\\\":\\\"提示缓存：10倍更便宜的 LLM Token，原理是什么？\\\",\\\"datePublished\\\":\\\"2025-12-31T00:00:00.000Z\\\",\\\"dateModified\\\":\\\"2025-12-31T00:00:00.000Z\\\",\\\"description\\\":\\\"深入解析 LLM 的提示缓存机制，了解 KV 缓存如何工作，为什么能节省成本并降低延迟，以及 OpenAI 和 Anthropic 不同的缓存策略。\\\",\\\"image\\\":\\\"https://pngimg.com/uploads/github/github_PNG80.png\\\",\\\"url\\\":\\\"https://blog.mainjay.cloudns.ch/blog/ai/prompt-caching\\\",\\\"author\\\":[{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"mainJayLai\\\"}]}\"}}],[\"$\",\"section\",null,{\"className\":\"mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0\",\"children\":[[\"$\",\"$L18\",null,{}],[\"$\",\"article\",null,{\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"header\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"space-y-1 border-b border-gray-200 pb-10 text-center dark:border-gray-700\",\"children\":[[\"$\",\"div\",null,{\"className\":\"beautiful-chinese-title\",\"children\":[\"$\",\"h1\",null,{\"className\":\"text-3xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 sm:text-4xl sm:leading-10 md:text-5xl md:leading-14\",\"children\":\"提示缓存：10倍更便宜的 LLM Token，原理是什么？\"}]}],[\"$\",\"dl\",null,{\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"dt\",null,{\"className\":\"sr-only\",\"children\":\"Published on\"}],[\"$\",\"dd\",null,{\"className\":\"text-base font-medium leading-6 text-gray-500 dark:text-gray-400\",\"children\":[\"$\",\"time\",null,{\"dateTime\":\"2025-12-31T00:00:00.000Z\",\"children\":\"December 31, 2025\"}]}]]}]}]]}]}],[\"$\",\"div\",null,{\"className\":\"grid-rows-[auto_1fr] divide-y divide-gray-200 pb-8 dark:divide-gray-700 xl:divide-y-0\",\"children\":[[\"$\",\"div\",null,{\"className\":\"divide-y divide-gray-200 dark:divide-gray-700 xl:col-span-3 xl:row-span-2 xl:pb-0\",\"children\":[\"$\",\"div\",null,{\"className\":\"beautiful-chinese-content prose max-w-none pb-8 pt-10 dark:prose-invert\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"目录\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#目录\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"目录\"]}],[\"$\",\"ul\",null,{\"className\":\"\",\"children\":[[\"$\",\"li\",\"0\",{\"className\":\"\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#引言\",\"children\":\"引言\"}],null]}],[\"$\",\"li\",\"1\",{\"className\":\"\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#llm-架构速览\",\"children\":\"LLM 架构速览\"}],null]}],[\"$\",\"li\",\"2\",{\"className\":\"\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#分词从文本到-token\",\"children\":\"分词：从文本到 Token\"}],null]}],[\"$\",\"li\",\"3\",{\"className\":\"\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#嵌入从-token-到向量\",\"children\":\"嵌入：从 Token 到向量\"}],null]}],[\"$\",\"li\",\"4\",{\"className\":\"\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#注意力机制缓存发生的地方\",\"children\":\"注意力机制：缓存发生的地方\"}],null]}],[\"$\",\"li\",\"5\",{\"className\":\"\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#kv-缓存真正的魔法\",\"children\":\"KV 缓存：真正的魔法\"}],null]}],[\"$\",\"li\",\"6\",{\"className\":\"\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#提示缓存跨请求的-kv-缓存\",\"children\":\"提示缓存：跨请求的 KV 缓存\"}],null]}],[\"$\",\"li\",\"7\",{\"className\":\"\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#openai-vs-anthropic不同的缓存策略\",\"children\":\"OpenAI vs Anthropic：不同的缓存策略\"}],null]}],[\"$\",\"li\",\"8\",{\"className\":\"\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#总结\",\"children\":\"总结\"}],null]}],[\"$\",\"li\",\"9\",{\"className\":\"\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#参考资料\",\"children\":\"参考资料\"}],null]}]]}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"引言\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#引言\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"引言\"]}],[\"$\",\"p\",null,{\"children\":\"使用大型语言模型 (LLM) 的 API 价格通常按 token 收费。每次你向 Claude、GPT-4 或其他模型发送请求时，都需要为输入 token（发送给模型的内容）和输出 token（模型返回的内容）付费。\"}],[\"$\",\"p\",null,{\"children\":\"但事情有个微妙之处。使用 API 时，你经常会发送包含大量重复内容的消息。也许你每次请求都附带了一个大型系统提示，或者你在对话进行中不断发送整个对话历史。无论哪种情况，你都在为多次处理相同的数据付费。\"}],[\"$\",\"p\",null,{\"children\":[\"为了解决这个问题，LLM 提供商开发了缓存功能。OpenAI 提供\",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://platform.openai.com/docs/guides/prompt-caching\",\"children\":\"提示缓存\"}],\"，Anthropic 则提供\",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\",\"children\":\"扩展缓存\"}],\"。两者都承诺为缓存的 token 提供高达 90% 的折扣以及更低的延迟。\"]}],[\"$\",\"p\",null,{\"children\":\"这就引出了一个问题：LLM 究竟是如何利用缓存的？什么数据被缓存了？为什么它能让处理更便宜？\"}],[\"$\",\"p\",null,{\"children\":\"本文将探讨这些问题，并让你深入理解这个机制的工作原理。\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"llm-架构速览\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#llm-架构速览\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"LLM 架构速览\"]}],[\"$\",\"p\",null,{\"children\":\"要理解提示缓存如何让 LLM 推理更快、更便宜，我们首先需要对 LLM 进行端到端的概览，考察 transformer 模型的各个步骤及其作用。我们可以将 LLM 分为四个阶段：\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"分词器 (Tokenizer)\"}],\"：将输入文本（字符串）转换为 token ID（整数）列表\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"嵌入 (Embedding)\"}],\"：将 token ID（整数）转换为向量（n 维空间中的浮点数列表）\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"变换器 (Transformer)\"}],\"：接收嵌入向量列表，输出另一个嵌入向量列表\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"输出 (Output)\"}],\"：将嵌入转回词汇表上的 token 概率\"]}]]}],[\"$\",\"p\",null,{\"children\":\"让我们逐一探索这些阶段，以便了解缓存实际发生的位置以及它如何提高性能。\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"分词从文本到-token\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#分词从文本到-token\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"分词：从文本到 Token\"]}],[\"$\",\"p\",null,{\"children\":[\"大语言模型并不直接处理我们读写的原始文本。相反，它们处理的是 \",[\"$\",\"strong\",null,{\"children\":\"token\"}],\"。token 可以是单个字符、词素或整个单词，这取决于模型所使用的分词器。\"]}],[\"$\",\"p\",null,{\"children\":[\"所有 token 被编译成一个 \",[\"$\",\"strong\",null,{\"children\":\"词汇表\"}],\"——模型知道的所有 token 的完整列表。词汇表通常有数万个条目，每个条目都有唯一的整数 ID。这个概念并不特别新奇：就像你从电子表格中导出数据时，可能会将类别列转换为类别 ID。\"]}],[\"$\",\"p\",null,{\"children\":\"为了让你直观感受这个概念：\"}],[\"$\",\"div\",null,{\"className\":\"w-full overflow-x-auto\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Token\"}],[\"$\",\"th\",null,{\"children\":\"Token ID\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"The\"}],[\"$\",\"td\",null,{\"children\":\"464\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"cat\"}],[\"$\",\"td\",null,{\"children\":\"2857\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"sat\"}],[\"$\",\"td\",null,{\"children\":\"8714\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"on\"}],[\"$\",\"td\",null,{\"children\":\"319\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"the\"}],[\"$\",\"td\",null,{\"children\":\"279\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"mat\"}],[\"$\",\"td\",null,{\"children\":\"5765\"}]]}]]}]]}]}],[\"$\",\"p\",null,{\"children\":\"当句子 \\\"The cat sat on the mat\\\" 经过分词器处理后：\"}],[\"$\",\"$L19\",null,{\"className\":\"language-js\",\"children\":[\"$\",\"code\",null,{\"className\":\"code-highlight language-js\",\"children\":[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"The cat sat on the mat\\\"\"}],\" → \",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"[\"}],[\"$\",\"span\",null,{\"className\":\"token number\",\"children\":\"464\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\" \",[\"$\",\"span\",null,{\"className\":\"token number\",\"children\":\"2857\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\" \",[\"$\",\"span\",null,{\"className\":\"token number\",\"children\":\"8714\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\" \",[\"$\",\"span\",null,{\"className\":\"token number\",\"children\":\"319\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\" \",[\"$\",\"span\",null,{\"className\":\"token number\",\"children\":\"279\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\" \",[\"$\",\"span\",null,{\"className\":\"token number\",\"children\":\"5765\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"]\"}],\"\\n\"]}]}]}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"嵌入从-token-到向量\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#嵌入从-token-到向量\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"嵌入：从 Token 到向量\"]}],[\"$\",\"p\",null,{\"children\":[\"一旦我们有了 token ID 序列，下一步就是将每个 ID 转换为 \",[\"$\",\"strong\",null,{\"children\":\"嵌入向量\"}],\"。\"]}],[\"$\",\"p\",null,{\"children\":\"嵌入是表示一段数据（在这里是一个 token）的实数向量。你可以把它想象成 token 语义含义的数学表示，捕获了模型所理解的关于这个 token 在语言中如何使用的一切。\"}],[\"$\",\"p\",null,{\"children\":[\"更直白地说：\",[\"$\",\"strong\",null,{\"children\":\"嵌入只是一个数字列表\"}],\"。每个 token 都有自己的嵌入，嵌入包含相同数量的数字（即维度）。现代 LLM 通常使用大约 1536 到 8192 个维度。\"]}],[\"$\",\"p\",null,{\"children\":\"例如，一个非常简化的 3 维嵌入可能是：\"}],[\"$\",\"div\",null,{\"className\":\"w-full overflow-x-auto\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Token\"}],[\"$\",\"th\",null,{\"children\":\"嵌入 (3维示例)\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"cat\"}],[\"$\",\"td\",null,{\"children\":\"[0.12, -0.45, 0.78]\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"dog\"}],[\"$\",\"td\",null,{\"children\":\"[0.11, -0.42, 0.80]\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"mat\"}],[\"$\",\"td\",null,{\"children\":\"[0.85, 0.23, -0.12]\"}]]}]]}]]}]}],[\"$\",\"p\",null,{\"children\":\"注意 \\\"cat\\\" 和 \\\"dog\\\" 的嵌入相似——因为它们在语义上相关（都是动物）。\\\"mat\\\" 则完全不同。实际的嵌入要复杂得多，但原理相同。\"}],[\"$\",\"p\",null,{\"children\":[\"到这个阶段，我们输入到 LLM 的句子从一个字符串变成了一个 \",[\"$\",\"strong\",null,{\"children\":\"嵌入矩阵\"}],\"：每行代表一个 token，列代表嵌入维度。\"]}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"注意力机制缓存发生的地方\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#注意力机制缓存发生的地方\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"注意力机制：缓存发生的地方\"]}],[\"$\",\"p\",null,{\"children\":[\"这就是精华所在。Transformer 的核心是 \",[\"$\",\"strong\",null,{\"children\":\"自注意力机制 (Self-Attention)\"}],\"，它允许模型中的每个 token 关注序列中的其他 token 并混合信息。这是 LLM 真正进行\\\"思考\\\"和\\\"推理\\\"的地方。\"]}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"qkv-矩阵\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#qkv-矩阵\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Q、K、V 矩阵\"]}],[\"$\",\"p\",null,{\"children\":[\"注意力机制使用三个关键概念：\",[\"$\",\"strong\",null,{\"children\":\"Query (查询)\"}],\"、\",[\"$\",\"strong\",null,{\"children\":\"Key (键)\"}],\" 和 \",[\"$\",\"strong\",null,{\"children\":\"Value (值)\"}],\"。\"]}],[\"$\",\"p\",null,{\"children\":\"可以这样理解：\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Query (Q)\"}],\"：代表\\\"我在寻找什么信息？\\\"\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Key (K)\"}],\"：代表\\\"我有什么信息可以提供？\\\"\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Value (V)\"}],\"：代表\\\"如果你问我，这是我实际提供的信息\\\"\"]}]]}],[\"$\",\"p\",null,{\"children\":\"对于输入中的每个 token 嵌入，我们通过与三个权重矩阵相乘来生成 Q、K、V 向量：\"}],[\"$\",\"p\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"Q\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"X\"}],[\"$\",\"mo\",null,{\"children\":\"⋅\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"W\"}],[\"$\",\"mi\",null,{\"children\":\"Q\"}]]}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"Q = X \\\\cdot W_Q\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".8778em\",\"verticalAlign\":\"-.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"Q\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".07847em\"},\"children\":\"X\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2222em\"}}],[\"$\",\"span\",null,{\"className\":\"mbin\",\"children\":\"⋅\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2222em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".9694em\",\"verticalAlign\":\"-.2861em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".13889em\"},\"children\":\"W\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".3283em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-.1389em\",\"marginRight\":\".05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"mtight sizing reset-size6 size3\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"Q\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".2861em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]]}]]}],\" \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"K\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"X\"}],[\"$\",\"mo\",null,{\"children\":\"⋅\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"W\"}],[\"$\",\"mi\",null,{\"children\":\"K\"}]]}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"K = X \\\\cdot W_K\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".07153em\"},\"children\":\"K\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".07847em\"},\"children\":\"X\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2222em\"}}],[\"$\",\"span\",null,{\"className\":\"mbin\",\"children\":\"⋅\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2222em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".8333em\",\"verticalAlign\":\"-.15em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".13889em\"},\"children\":\"W\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".3283em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-.1389em\",\"marginRight\":\".05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"mtight sizing reset-size6 size3\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\".07153em\"},\"children\":\"K\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]]}]]}],\" \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"V\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"X\"}],[\"$\",\"mo\",null,{\"children\":\"⋅\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"W\"}],[\"$\",\"mi\",null,{\"children\":\"V\"}]]}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"V = X \\\\cdot W_V\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".22222em\"},\"children\":\"V\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".07847em\"},\"children\":\"X\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2222em\"}}],[\"$\",\"span\",null,{\"className\":\"mbin\",\"children\":\"⋅\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2222em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".8333em\",\"verticalAlign\":\"-.15em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".13889em\"},\"children\":\"W\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".3283em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-.1389em\",\"marginRight\":\".05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"mtight sizing reset-size6 size3\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\".22222em\"},\"children\":\"V\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]]}]]}]]}],[\"$\",\"p\",null,{\"children\":[\"其中 \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"X\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"X\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".07847em\"},\"children\":\"X\"}]]}]}]]}],\" 是输入嵌入矩阵，\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"W\"}],[\"$\",\"mi\",null,{\"children\":\"Q\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"W_Q\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".9694em\",\"verticalAlign\":\"-.2861em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".13889em\"},\"children\":\"W\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".3283em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-.1389em\",\"marginRight\":\".05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"mtight sizing reset-size6 size3\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"Q\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".2861em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\"、\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"W\"}],[\"$\",\"mi\",null,{\"children\":\"K\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"W_K\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".8333em\",\"verticalAlign\":\"-.15em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".13889em\"},\"children\":\"W\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".3283em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-.1389em\",\"marginRight\":\".05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"mtight sizing reset-size6 size3\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\".07153em\"},\"children\":\"K\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\"、\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"W\"}],[\"$\",\"mi\",null,{\"children\":\"V\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"W_V\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".8333em\",\"verticalAlign\":\"-.15em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".13889em\"},\"children\":\"W\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".3283em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-.1389em\",\"marginRight\":\".05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"mtight sizing reset-size6 size3\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\".22222em\"},\"children\":\"V\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\" 是模型在训练过程中学到的权重矩阵。\"]}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"注意力分数计算\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#注意力分数计算\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"注意力分数计算\"]}],[\"$\",\"p\",null,{\"children\":\"计算出 Q、K、V 后，注意力机制执行以下计算：\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mtext\",null,{\"children\":\"Attention\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"Q\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mi\",null,{\"children\":\"K\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mi\",null,{\"children\":\"V\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mtext\",null,{\"children\":\"softmax\"}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mo\",null,{\"fence\":\"true\",\"children\":\"(\"}],[\"$\",\"mfrac\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"Q\"}],[\"$\",\"msup\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"K\"}],[\"$\",\"mi\",null,{\"children\":\"T\"}]]}]]}],[\"$\",\"msqrt\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"d\"}],[\"$\",\"mi\",null,{\"children\":\"k\"}]]}]}]]}],[\"$\",\"mo\",null,{\"fence\":\"true\",\"children\":\")\"}]]}],[\"$\",\"mi\",null,{\"children\":\"V\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\text{Attention}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{QK^T}{\\\\sqrt{d_k}}\\\\right) V\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mord text\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"Attention\"}]}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"Q\"}],[\"$\",\"span\",null,{\"className\":\"mpunct\",\"children\":\",\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".07153em\"},\"children\":\"K\"}],[\"$\",\"span\",null,{\"className\":\"mpunct\",\"children\":\",\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".22222em\"},\"children\":\"V\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1.8em\",\"verticalAlign\":\"-.65em\"}}],[\"$\",\"span\",null,{\"className\":\"mord text\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"softmax\"}]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".1667em\"}}],[\"$\",\"span\",null,{\"className\":\"minner\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mopen delimcenter\",\"style\":{\"top\":\"0\"},\"children\":[\"$\",\"span\",null,{\"className\":\"delimsizing size2\",\"children\":\"(\"}]}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mopen nulldelimiter\"}],[\"$\",\"span\",null,{\"className\":\"mfrac\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"1.0895em\"},\"children\":[[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.5864em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"3em\"}}],[\"$\",\"span\",null,{\"className\":\"mtight sizing reset-size6 size3\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight sqrt\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".8622em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"svg-align\",\"style\":{\"top\":\"-3em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"3em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"style\":{\"paddingLeft\":\".833em\"},\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"d\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".3448em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.3488em\",\"marginLeft\":\"0\",\"marginRight\":\".0714em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.5em\"}}],[\"$\",\"span\",null,{\"className\":\"mtight sizing reset-size3 size1\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\".03148em\"},\"children\":\"k\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".1512em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]}]]}],[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.8222em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"3em\"}}],[\"$\",\"span\",null,{\"className\":\"mtight hide-tail\",\"style\":{\"minWidth\":\".853em\",\"height\":\"1.08em\"},\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 400000 1080\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"height\":\"1.08em\",\"preserveAspectRatio\":\"xMinYMin slice\",\"width\":\"400em\",\"children\":[\"$\",\"path\",null,{\"d\":\"M95,702\\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\\nc69,-144,104.5,-217.7,106.5,-221\\nl0 -0\\nc5.3,-9.3,12,-14,20,-14\\nH400000v40H845.2724\\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\\nM834 80h400000v40h-400000z\"}]}]}]]}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".1778em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]}]}]]}],[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.23em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"3em\"}}],[\"$\",\"span\",null,{\"className\":\"frac-line\",\"style\":{\"borderBottomWidth\":\".04em\"}}]]}],[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.4461em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"3em\"}}],[\"$\",\"span\",null,{\"className\":\"mtight sizing reset-size6 size3\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"Q\"}],[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\".07153em\"},\"children\":\"K\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".9191em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.931em\",\"marginRight\":\".0714em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.5em\"}}],[\"$\",\"span\",null,{\"className\":\"mtight sizing reset-size3 size1\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\".13889em\"},\"children\":\"T\"}]}]]}]}]}]}]}]]}]]}]}]]}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".538em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"mclose nulldelimiter\"}]]}],[\"$\",\"span\",null,{\"className\":\"mclose delimcenter\",\"style\":{\"top\":\"0\"},\"children\":[\"$\",\"span\",null,{\"className\":\"delimsizing size2\",\"children\":\")\"}]}]]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".22222em\"},\"children\":\"V\"}]]}]]}]]}]}],[\"$\",\"p\",null,{\"children\":\"这个公式做了什么？\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"Q\"}],[\"$\",\"msup\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"K\"}],[\"$\",\"mi\",null,{\"children\":\"T\"}]]}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"QK^T\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1.0358em\",\"verticalAlign\":\"-.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"Q\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".07153em\"},\"children\":\"K\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".8413em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.063em\",\"marginRight\":\".05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"mtight sizing reset-size6 size3\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\".13889em\"},\"children\":\"T\"}]}]]}]}]}]}]}]]}]]}]}]]}]}],\"：计算每个 query 与每个 key 的点积，得到注意力分数\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msqrt\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"d\"}],[\"$\",\"mi\",null,{\"children\":\"k\"}]]}]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\sqrt{d_k}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1.04em\",\"verticalAlign\":\"-.1828em\"}}],[\"$\",\"span\",null,{\"className\":\"mord sqrt\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".8572em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"svg-align\",\"style\":{\"top\":\"-3em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"3em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"style\":{\"paddingLeft\":\".833em\"},\"children\":[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"d\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".3361em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"0\",\"marginRight\":\".05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"mtight sizing reset-size6 size3\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\".03148em\"},\"children\":\"k\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]}]]}],[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.8172em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"3em\"}}],[\"$\",\"span\",null,{\"className\":\"hide-tail\",\"style\":{\"minWidth\":\".853em\",\"height\":\"1.08em\"},\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 400000 1080\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"height\":\"1.08em\",\"preserveAspectRatio\":\"xMinYMin slice\",\"width\":\"400em\",\"children\":[\"$\",\"path\",null,{\"d\":\"M95,702\\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\\nc69,-144,104.5,-217.7,106.5,-221\\nl0 -0\\nc5.3,-9.3,12,-14,20,-14\\nH400000v40H845.2724\\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\\nM834 80h400000v40h-400000z\"}]}]}]]}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".1828em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]}]]}]}],\"：缩放因子，防止分数过大\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"softmax\"}],\"：将分数转换为概率分布（和为 1）\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":[\"乘以 \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"V\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"V\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".22222em\"},\"children\":\"V\"}]]}]}]]}]]}],\"：用注意力权重加权求和所有 value\"]}]]}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"因果掩码\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#因果掩码\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"因果掩码\"]}],[\"$\",\"p\",null,{\"children\":[\"对于生成式 LLM（如 GPT、Claude），还有一个关键细节：\",[\"$\",\"strong\",null,{\"children\":\"因果掩码 (Causal Mask)\"}],\"。\"]}],[\"$\",\"p\",null,{\"children\":[\"在生成文本时，模型一次生成一个 token。关键规则是：\",[\"$\",\"strong\",null,{\"children\":\"每个 token 只能看到它之前的 token\"}],\"，不能\\\"偷看\\\"未来的 token。\"]}],[\"$\",\"p\",null,{\"children\":[\"为了实现这一点，在计算 \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"Q\"}],[\"$\",\"msup\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"K\"}],[\"$\",\"mi\",null,{\"children\":\"T\"}]]}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"QK^T\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1.0358em\",\"verticalAlign\":\"-.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"Q\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".07153em\"},\"children\":\"K\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".8413em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.063em\",\"marginRight\":\".05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"mtight sizing reset-size6 size3\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\".13889em\"},\"children\":\"T\"}]}]]}]}]}]}]}]]}]]}]}]]}],\" 后、应用 softmax 之前，我们添加一个下三角掩码，将未来位置的分数设为负无穷。负无穷经过 softmax 后变成 0，有效地阻止了信息从未来流向过去。\"]}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"kv-缓存真正的魔法\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#kv-缓存真正的魔法\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"KV 缓存：真正的魔法\"]}],[\"$\",\"p\",null,{\"children\":\"现在我们终于可以理解提示缓存的核心机制了。\"}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"为什么需要-kv-缓存\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#为什么需要-kv-缓存\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"为什么需要 KV 缓存？\"]}],[\"$\",\"p\",null,{\"children\":\"考虑这样一个场景：你正在与 LLM 进行对话，每次发送新消息时都需要包含之前的对话历史。\"}],[\"$\",\"p\",null,{\"children\":\"假设对话历史有 1000 个 token，你新增了 10 个 token。传统方式下，模型需要：\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"对所有 1010 个 token 重新计算 Q、K、V 矩阵\"}],[\"$\",\"li\",null,{\"children\":\"计算完整的注意力分数\"}],[\"$\",\"li\",null,{\"children\":\"生成输出\"}]]}],[\"$\",\"p\",null,{\"children\":[\"但这里有个洞察：\",[\"$\",\"strong\",null,{\"children\":\"之前 1000 个 token 的 K 和 V 矩阵不会改变\"}],\"！\"]}],[\"$\",\"p\",null,{\"children\":\"因为计算 K 和 V 只依赖于 token 本身和固定的权重矩阵。只要输入 token 相同，输出的 K、V 就相同。\"}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"kv-缓存如何工作\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#kv-缓存如何工作\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"KV 缓存如何工作\"]}],[\"$\",\"p\",null,{\"children\":\"有了这个洞察，KV 缓存的思路就很清晰了：\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"首次处理提示时\"}],\"：计算并存储所有 token 的 K 和 V 矩阵\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"生成新 token 时\"}],\"：只为新 token 计算 K 和 V，然后将其追加到缓存中\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"计算注意力时\"}],\"：新 token 的 Q 与所有缓存的 K 计算注意力分数，然后用这些分数加权所有缓存的 V\"]}]]}],[\"$\",\"p\",null,{\"children\":\"这带来了巨大的计算节省：\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"无缓存\"}],\"：每生成一个新 token，都要重新处理所有之前的 token\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"有缓存\"}],\"：每生成一个新 token，只需处理那一个 token\"]}]]}],[\"$\",\"p\",null,{\"children\":[\"复杂度从 \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"O\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"msup\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"n\"}],[\"$\",\"mn\",null,{\"children\":\"2\"}]]}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"O(n^2)\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1.0641em\",\"verticalAlign\":\"-.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".02778em\"},\"children\":\"O\"}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"n\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".8141em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.063em\",\"marginRight\":\".05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"mtight sizing reset-size6 size3\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"2\"}]}]]}]}]}]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}]]}]}]]}],\" 降低到了 \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"O\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"n\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"O(n)\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\".02778em\"},\"children\":\"O\"}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"n\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}]]}]}]]}],\"，其中 \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"n\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"n\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".4306em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"n\"}]]}]}]]}],\" 是序列长度。\"]}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"实际数据量\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#实际数据量\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"实际数据量\"]}],[\"$\",\"p\",null,{\"children\":\"让我们计算一下缓存了多少数据。假设：\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"模型维度 \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"d\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mn\",null,{\"children\":\"4096\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"d = 4096\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".6944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"d\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".6444em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"4096\"}]]}]]}]]}],\"（常见于 GPT-4 级别模型）\"]}],[\"$\",\"li\",null,{\"children\":\"32 层\"}],[\"$\",\"li\",null,{\"children\":\"32 个注意力头\"}],[\"$\",\"li\",null,{\"children\":[\"每个头维度 \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"d\"}],[\"$\",\"mi\",null,{\"children\":\"k\"}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mn\",null,{\"children\":\"128\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"d_k = 128\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".8444em\",\"verticalAlign\":\"-.15em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"d\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".3361em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"0\",\"marginRight\":\".05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"mtight sizing reset-size6 size3\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\".03148em\"},\"children\":\"k\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\".15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\".2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\".6444em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"128\"}]]}]]}]]}]]}]]}],[\"$\",\"p\",null,{\"children\":\"对于 1024 个 token 的缓存：\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"每个 token：K 和 V 各 4096 个浮点数 × 32 层 = 262,144 个浮点数\"}],[\"$\",\"li\",null,{\"children\":\"1024 个 token：约 2.68 亿个浮点数\"}],[\"$\",\"li\",null,{\"children\":\"以 FP16 存储：约 512 MB\"}]]}],[\"$\",\"p\",null,{\"children\":\"这就是为什么 LLM 推理需要大量 GPU 显存——缓存本身就占用了相当大的空间。\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"提示缓存跨请求的-kv-缓存\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#提示缓存跨请求的-kv-缓存\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"提示缓存：跨请求的 KV 缓存\"]}],[\"$\",\"p\",null,{\"children\":[\"标准的 KV 缓存只在单次请求内有效。但 LLM 提供商更进一步：\",[\"$\",\"strong\",null,{\"children\":\"跨请求持久化缓存\"}],\"。\"]}],[\"$\",\"p\",null,{\"children\":\"这就是 OpenAI 和 Anthropic 的\\\"提示缓存\\\"功能。\"}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"工作原理\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#工作原理\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"工作原理\"]}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"缓存键\"}],\"：系统使用提示内容的哈希值作为缓存键\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"缓存匹配\"}],\"：新请求到来时，检查提示前缀是否与缓存匹配\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"缓存命中\"}],\"：如果匹配，直接使用缓存的 K、V 矩阵，跳过计算\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"缓存失效\"}],\"：经过一段时间后缓存自动失效（通常 5-10 分钟）\"]}]]}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"为什么能省钱\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#为什么能省钱\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"为什么能省钱？\"]}],[\"$\",\"p\",null,{\"children\":\"计算 K、V 矩阵是 LLM 推理中最昂贵的部分之一。当你的请求命中缓存时：\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"减少计算\"}],\"：GPU 不需要为缓存的 token 进行矩阵运算\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"降低延迟\"}],\"：直接使用预计算的结果\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"节省成本\"}],\"：提供商将节省的成本以折扣形式传递给用户\"]}]]}],[\"$\",\"p\",null,{\"children\":\"这就是为什么缓存的 token 可以便宜 50-90%。\"}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"openai-vs-anthropic不同的缓存策略\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#openai-vs-anthropic不同的缓存策略\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"OpenAI vs Anthropic：不同的缓存策略\"]}],[\"$\",\"p\",null,{\"children\":\"虽然底层原理相同，但两家公司的实现策略有显著差异。\"}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"openai自动缓存\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#openai自动缓存\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"OpenAI：自动缓存\"]}],[\"$\",\"p\",null,{\"children\":[\"OpenAI 的提示缓存是 \",[\"$\",\"strong\",null,{\"children\":\"完全自动\"}],\" 的：\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"自动检测可缓存的提示前缀\"}],[\"$\",\"li\",null,{\"children\":\"对开发者完全透明\"}],[\"$\",\"li\",null,{\"children\":\"缓存持续约 5-10 分钟\"}],[\"$\",\"li\",null,{\"children\":\"最小缓存单位：1024 tokens\"}],[\"$\",\"li\",null,{\"children\":\"缓存命中时享受 50% 折扣\"}]]}],[\"$\",\"$L19\",null,{\"className\":\"language-python\",\"children\":[\"$\",\"code\",null,{\"className\":\"language-python code-highlight\",\"children\":[[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[[\"$\",\"span\",null,{\"className\":\"token comment\",\"children\":\"# OpenAI 无需任何改动，缓存自动工作\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"response \",[\"$\",\"span\",null,{\"className\":\"token operator\",\"children\":\"=\"}],\" client\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\".\"}],\"chat\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\".\"}],\"completions\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\".\"}],\"create\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"(\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"    model\",[\"$\",\"span\",null,{\"className\":\"token operator\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"gpt-4\\\"\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"    messages\",[\"$\",\"span\",null,{\"className\":\"token operator\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"[\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"        \",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"{\"}],[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"role\\\"\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\":\"}],\" \",[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"system\\\"\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\" \",[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"content\\\"\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\":\"}],\" very_long_system_prompt\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"}\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"        \",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"{\"}],[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"role\\\"\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\":\"}],\" \",[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"user\\\"\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\" \",[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"content\\\"\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\":\"}],\" user_message\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"}\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"    \",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"]\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\")\"}],\"\\n\"]}]]}]}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"anthropic显式控制\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#anthropic显式控制\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Anthropic：显式控制\"]}],[\"$\",\"p\",null,{\"children\":[\"Anthropic 让开发者 \",[\"$\",\"strong\",null,{\"children\":\"显式标记\"}],\" 要缓存的内容：\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"使用 \",[\"$\",\"code\",null,{\"className\":\"custom-code\",\"children\":\"cache_control\"}],\" 参数指定缓存点\"]}],[\"$\",\"li\",null,{\"children\":\"最多可设置 4 个缓存断点\"}],[\"$\",\"li\",null,{\"children\":\"更细粒度的控制\"}],[\"$\",\"li\",null,{\"children\":\"缓存命中时享受 90% 折扣\"}],[\"$\",\"li\",null,{\"children\":\"写入缓存时有额外费用（约 25%）\"}]]}],[\"$\",\"$L19\",null,{\"className\":\"language-python\",\"children\":[\"$\",\"code\",null,{\"className\":\"language-python code-highlight\",\"children\":[[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[[\"$\",\"span\",null,{\"className\":\"token comment\",\"children\":\"# Anthropic 需要显式标记缓存\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"response \",[\"$\",\"span\",null,{\"className\":\"token operator\",\"children\":\"=\"}],\" client\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\".\"}],\"messages\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\".\"}],\"create\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"(\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"    model\",[\"$\",\"span\",null,{\"className\":\"token operator\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"claude-sonnet-4-20250514\\\"\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"    system\",[\"$\",\"span\",null,{\"className\":\"token operator\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"[\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"        \",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"{\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"            \",[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"type\\\"\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\":\"}],\" \",[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"text\\\"\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"            \",[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"text\\\"\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\":\"}],\" very_long_system_prompt\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"            \",[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"cache_control\\\"\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\":\"}],\" \",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"{\"}],[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"type\\\"\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\":\"}],\" \",[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"ephemeral\\\"\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"}\"}],\"  \",[\"$\",\"span\",null,{\"className\":\"token comment\",\"children\":\"# 显式缓存\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"        \",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"}\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"    \",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"]\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"    messages\",[\"$\",\"span\",null,{\"className\":\"token operator\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"[\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"{\"}],[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"role\\\"\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\":\"}],\" \",[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"user\\\"\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\" \",[\"$\",\"span\",null,{\"className\":\"token string\",\"children\":\"\\\"content\\\"\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\":\"}],\" user_message\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"}\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"]\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\")\"}],\"\\n\"]}]]}]}],[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"如何选择\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#如何选择\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"如何选择？\"]}],[\"$\",\"div\",null,{\"className\":\"w-full overflow-x-auto\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"场景\"}],[\"$\",\"th\",null,{\"children\":\"OpenAI 方案\"}],[\"$\",\"th\",null,{\"children\":\"Anthropic 方案\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"简单用例\"}],[\"$\",\"td\",null,{\"children\":\"零配置，自动生效\"}],[\"$\",\"td\",null,{\"children\":\"需要手动添加缓存标记\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"大型系统提示\"}],[\"$\",\"td\",null,{\"children\":\"自动缓存前缀\"}],[\"$\",\"td\",null,{\"children\":\"可精确控制缓存边界\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"成本优化\"}],[\"$\",\"td\",null,{\"children\":\"50% 折扣，较简单\"}],[\"$\",\"td\",null,{\"children\":\"90% 折扣，但有写入成本\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"复杂工作流\"}],[\"$\",\"td\",null,{\"children\":\"控制较少\"}],[\"$\",\"td\",null,{\"children\":\"更灵活的缓存策略\"}]]}]]}]]}]}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"总结\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#总结\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"总结\"]}],[\"$\",\"p\",null,{\"children\":\"提示缓存的核心原理可以总结为：\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"LLM 推理的瓶颈\"}],\"：计算注意力机制中的 K、V 矩阵非常昂贵\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"KV 缓存的洞察\"}],\"：对于相同的输入 token，K、V 矩阵的计算结果相同\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"跨请求持久化\"}],\"：将 KV 缓存扩展到多个 API 请求之间共享\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"成本节省\"}],\"：跳过重复计算 = 更少的 GPU 时间 = 更低的价格\"]}]]}],[\"$\",\"p\",null,{\"children\":\"理解这个机制后，你可以：\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"优化提示结构\"}],\"：将静态内容放在前面，动态内容放在后面\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"合理使用系统提示\"}],\"：大型系统提示是缓存的理想候选\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"选择合适的提供商\"}],\"：根据你的需求选择自动或手动缓存策略\"]}]]}],[\"$\",\"p\",null,{\"children\":\"下次当你看到 API 账单上的\\\"缓存 token\\\"折扣时，你就知道背后发生了什么了。\"}],[\"$\",\"hr\",null,{}],[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"参考资料\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#参考资料\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"参考资料\"]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://ngrok.com/blog/prompt-caching\",\"children\":\"Prompt caching: 10x cheaper LLM tokens, but how?\"}],\" - Sam Rose, ngrok\"]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://platform.openai.com/docs/guides/prompt-caching\",\"children\":\"OpenAI Prompt Caching Guide\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\",\"children\":\"Anthropic Prompt Caching Documentation\"}]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://arxiv.org/abs/1706.03762\",\"children\":\"Attention Is All You Need\"}],\" - Vaswani et al.\"]}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"pb-6 pt-6 text-center text-gray-700 dark:text-gray-300\",\"id\":\"comment\",\"children\":[\"$\",\"$L1a\",null,{\"slug\":\"ai/prompt-caching\"}]}],[\"$\",\"footer\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col text-sm font-medium sm:flex-row sm:justify-between sm:text-base\",\"children\":[[\"$\",\"div\",null,{\"className\":\"pt-4 xl:pt-8\",\"children\":[\"$\",\"$Le\",null,{\"href\":\"/blog/ai/tokenization-pipeline\",\"className\":\"text-primary-500 hover:text-primary-600 dark:hover:text-primary-400\",\"aria-label\":\"Previous post: 从文本到词元：分词管道的工作原理\",\"children\":[\"← \",\"从文本到词元：分词管道的工作原理\"]}]}],\"$undefined\"]}]}]]}]]}]}]]}]]\n"])</script><script>self.__next_f.push([1,"15:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"提示缓存：10倍更便宜的 LLM Token，原理是什么？\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"深入解析 LLM 的提示缓存机制，了解 KV 缓存如何工作，为什么能节省成本并降低延迟，以及 OpenAI 和 Anthropic 不同的缓存策略。\"}],[\"$\",\"meta\",\"4\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"5\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"6\",{\"rel\":\"canonical\",\"href\":\"https://blog.mainjay.cloudns.ch/blog/ai/prompt-caching\"}],[\"$\",\"link\",\"7\",{\"rel\":\"alternate\",\"type\":\"application/rss+xml\",\"href\":\"https://blog.mainjay.cloudns.ch/feed.xml\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:title\",\"content\":\"提示缓存：10倍更便宜的 LLM Token，原理是什么？\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:description\",\"content\":\"深入解析 LLM 的提示缓存机制，了解 KV 缓存如何工作，为什么能节省成本并降低延迟，以及 OpenAI 和 Anthropic 不同的缓存策略。\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:url\",\"content\":\"https://blog.mainjay.cloudns.ch/blog/ai/prompt-caching\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:site_name\",\"content\":\"MainJayLai Blog\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:image\",\"content\":\"https://pngimg.com/uploads/github/github_PNG80.png\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"15\",{\"property\":\"article:published_time\",\"content\":\"2025-12-31T00:00:00.000Z\"}],[\"$\",\"meta\",\"16\",{\"property\":\"article:modified_time\",\"content\":\"2025-12-31T00:00:00.000Z\"}],[\"$\",\"meta\",\"17\",{\"property\":\"article:author\",\"content\":\"mainJayLai\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"19\",{\"name\":\"twitter:title\",\"content\":\"提示缓存：10倍更便宜的 LLM Token，原理是什么？\"}],[\"$\",\"meta\",\"20\",{\"name\":\"twitter:description\",\"content\":\"深入解析 LLM 的提示缓存机制，了解 KV 缓存如何工作，为什么能节省成本并降低延迟，以及 OpenAI 和 Anthropic 不同的缓存策略。\"}],[\"$\",\"meta\",\"21\",{\"name\":\"twitter:image\",\"content\":\"https://pngimg.com/uploads/github/github_PNG80.png\"}],[\"$\",\"meta\",\"22\",{\"name\":\"next-size-adjust\"}]]\n"])</script><script>self.__next_f.push([1,"6:null\n"])</script></body></html>